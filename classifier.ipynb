{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify RNA-Seq + ATAC-Seq by Cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:05:27) \n",
      "[Clang 9.0.1 ]\n",
      "TensorFlow version: 2.1.0\n",
      "\n",
      "Imports Complete.\n"
     ]
    }
   ],
   "source": [
    "# import needed modules:\n",
    "\n",
    "# general python utilities\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import functools\n",
    "import itertools as it\n",
    "import copy\n",
    "\n",
    "# recommended Python3 version >= 3.5\n",
    "print('Python version: {}'.format(platform.sys.version))\n",
    "\n",
    "# data-science & processing tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import h5py\n",
    "\n",
    "# progress bar\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# plotting utilities\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "\n",
    "# required TensorFlow version >= 2.0.0\n",
    "tf_version = tf.__version__\n",
    "print('TensorFlow version: {}'.format(tf_version))\n",
    "assert int(tf_version[0]) >= 2, \"Tensorflow version must be >= 2.0\"\n",
    "\n",
    "# seed random numbers for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "print('\\nImports Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some fake data until real data becomes available\n",
    "n_samples, n_rna_features, n_atac_features = 6000, 500, 100\n",
    "rna_seq_fake = np.random.rand(n_samples, n_rna_features)\n",
    "atac_seq_fake = np.random.rand(n_samples, n_atac_features)\n",
    "X_np = np.concatenate([rna_seq_fake, atac_seq_fake], axis=1)\n",
    "Y_np = np.array(['cell_type_A'] * 2000 + ['cell_type_B'] * 2000 + ['cell_type_C'] * 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6000, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make one-hot encoding of cell type classes\n",
    "le = LabelEncoder()\n",
    "Y_le = le.fit_transform(Y_np)\n",
    "tf.one_hot(Y_le, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load in matrices generated after pre-processing steps '''\n",
    "PATH = os.getcwd() # make sure cwd is 6_874-Multimodal-DL\n",
    "OUTPUTS = os.path.join(PATH, 'Outputs')\n",
    "atac_data_path = os.path.join(OUTPUTS, 'atacMatrix1')\n",
    "rna_data_path = os.path.join(OUTPUTS, 'rnaMatrix1')\n",
    "bimodal_data_path = os.path.join(OUTPUTS, 'bimodalEarlyFusionMatrix1')\n",
    "\n",
    "# save as pandas dataframes\n",
    "atac_df = pd.read_hdf(atac_data_path)\n",
    "rna_df = pd.read_hdf(rna_data_path)\n",
    "bimodal_df = pd.read_hdf(bimodal_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>matrix</th>\n",
       "      <th>treatment_time</th>\n",
       "      <th>atac_1</th>\n",
       "      <th>atac_2</th>\n",
       "      <th>atac_4</th>\n",
       "      <th>atac_10</th>\n",
       "      <th>atac_17</th>\n",
       "      <th>atac_22</th>\n",
       "      <th>atac_24</th>\n",
       "      <th>...</th>\n",
       "      <th>atac_189594</th>\n",
       "      <th>atac_189595</th>\n",
       "      <th>atac_189596</th>\n",
       "      <th>atac_189597</th>\n",
       "      <th>atac_189598</th>\n",
       "      <th>atac_189599</th>\n",
       "      <th>atac_189600</th>\n",
       "      <th>atac_189601</th>\n",
       "      <th>atac_189602</th>\n",
       "      <th>atac_189603</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sci-RNA-A-001.AAGTACGTTA</td>\n",
       "      <td>829</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.29541</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.031647</td>\n",
       "      <td>-0.048462</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.032776</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.029755</td>\n",
       "      <td>-0.015327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sci-RNA-A-001.CGTATTGAGA</td>\n",
       "      <td>830</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.29541</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.031647</td>\n",
       "      <td>-0.048462</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.032776</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.029755</td>\n",
       "      <td>-0.015327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sci-RNA-A-001.CGTCTATGAA</td>\n",
       "      <td>833</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.29541</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.031647</td>\n",
       "      <td>-0.048462</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.032776</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.029755</td>\n",
       "      <td>-0.015327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sci-RNA-A-001.GACCAATGCG</td>\n",
       "      <td>828</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.29541</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.031647</td>\n",
       "      <td>-0.048462</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.032776</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.029755</td>\n",
       "      <td>-0.015327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sci-RNA-A-001.TAGCCAGCAA</td>\n",
       "      <td>826</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.29541</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02655</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.031647</td>\n",
       "      <td>-0.048462</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.032776</td>\n",
       "      <td>-0.02504</td>\n",
       "      <td>-0.029755</td>\n",
       "      <td>-0.015327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150481 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sample  matrix  treatment_time   atac_1    atac_2  \\\n",
       "0  sci-RNA-A-001.AAGTACGTTA     829               3 -0.29541 -0.015327   \n",
       "1  sci-RNA-A-001.CGTATTGAGA     830               3 -0.29541 -0.015327   \n",
       "2  sci-RNA-A-001.CGTCTATGAA     833               1 -0.29541 -0.015327   \n",
       "3  sci-RNA-A-001.GACCAATGCG     828               0 -0.29541 -0.015327   \n",
       "4  sci-RNA-A-001.TAGCCAGCAA     826               3 -0.29541 -0.015327   \n",
       "\n",
       "     atac_4   atac_10   atac_17  atac_22   atac_24  ...  atac_189594  \\\n",
       "0 -0.015327 -0.015327 -0.015327 -0.02655 -0.015327  ...     -0.02655   \n",
       "1 -0.015327 -0.015327 -0.015327 -0.02655 -0.015327  ...     -0.02655   \n",
       "2 -0.015327 -0.015327 -0.015327 -0.02655 -0.015327  ...     -0.02655   \n",
       "3 -0.015327 -0.015327 -0.015327 -0.02655 -0.015327  ...     -0.02655   \n",
       "4 -0.015327 -0.015327 -0.015327 -0.02655 -0.015327  ...     -0.02655   \n",
       "\n",
       "   atac_189595  atac_189596  atac_189597  atac_189598  atac_189599  \\\n",
       "0    -0.049408    -0.018692    -0.031647    -0.048462     -0.02504   \n",
       "1    -0.049408    -0.018692    -0.031647    -0.048462     -0.02504   \n",
       "2    -0.049408    -0.018692    -0.031647    -0.048462     -0.02504   \n",
       "3    -0.049408    -0.018692    -0.031647    -0.048462     -0.02504   \n",
       "4    -0.049408    -0.018692    -0.031647    -0.048462     -0.02504   \n",
       "\n",
       "   atac_189600  atac_189601  atac_189602  atac_189603  \n",
       "0    -0.032776     -0.02504    -0.029755    -0.015327  \n",
       "1    -0.032776     -0.02504    -0.029755    -0.015327  \n",
       "2    -0.032776     -0.02504    -0.029755    -0.015327  \n",
       "3    -0.032776     -0.02504    -0.029755    -0.015327  \n",
       "4    -0.032776     -0.02504    -0.029755    -0.015327  \n",
       "\n",
       "[5 rows x 150481 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atac_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "- There are over 20,000 features, but only 3260 samples. Consider using PCA or another dimensionality reduction method before inputing into network\n",
    "- This matrix is super sparse. Think about how to account for that (maybe in feature selection)\n",
    "- TODO - feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get labels'''\n",
    "Y_atac = atac_df['treatment_time']\n",
    "Y_rna = rna_df['treatment_time']\n",
    "Y_bimodal = bimodal_df['treatment_time_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Change to label-encoded values'''\n",
    "le = LabelEncoder()\n",
    "Y_atac_le = le.fit_transform(Y_atac) # note that this changes treatment time of 3 to 2, but other labels are the same\n",
    "Y_rna_le = le.fit_transform(Y_rna)\n",
    "Y_bimodal_le = le.fit_transform(Y_bimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get feature labels '''\n",
    "# get features\n",
    "atac_features = atac_df.loc[:, 'atac_1':].columns\n",
    "rna_features = rna_df.loc[:, 'rna_2':].columns\n",
    "bimodal_features = bimodal_df.loc[:, 'rna_2':].columns\n",
    "\n",
    "# get number of features\n",
    "n_atac_features = len(atac_features)\n",
    "n_rna_features = len(rna_features)\n",
    "n_bi_features = len(bimodal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Make matrices into numpy arrays'''\n",
    "X_atac_np = atac_df.loc[:, 'atac_1':].to_numpy()\n",
    "X_rna_np = rna_df.loc[:, 'rna_2':].to_numpy()\n",
    "X_bimodal_np = bimodal_df.loc[:, 'rna_2':].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get training and test sets '''\n",
    "# TODO - make sure these are stratified\n",
    "X_train_atac, X_test_atac, Y_train_atac, Y_test_atac = train_test_split(X_atac_np, Y_atac_le, test_size = 0.1)#, stratify=[0,1,2])\n",
    "X_train_rna, X_test_rna, Y_train_rna, Y_test_rna = train_test_split(X_rna_np, Y_rna_le, test_size = 0.1)#, stratify=[0,1,2])\n",
    "X_train_bi, X_test_bi, Y_train_bi, Y_test_bi = train_test_split(X_bimodal_np, Y_bimodal_le, test_size = 0.1)#,stratify=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Make Y labels one-hot '''\n",
    "# training sets\n",
    "Y_atac_train = tf.one_hot(Y_train_atac, depth=3)\n",
    "Y_rna_train = tf.one_hot(Y_train_rna, depth=3)\n",
    "Y_bimodal_train = tf.one_hot(Y_train_bi, depth=3)\n",
    "\n",
    "# test sets\n",
    "Y_atac_test = tf.one_hot(Y_test_atac, depth=3)\n",
    "Y_rna_test = tf.one_hot(Y_test_rna, depth=3)\n",
    "Y_bimodal_test = tf.one_hot(Y_test_bi, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Make matrices into TF tensors'''\n",
    "# training sets\n",
    "X_atac_train = tf.convert_to_tensor(X_train_atac)\n",
    "X_rna_train = tf.convert_to_tensor(X_train_rna)\n",
    "X_bimodal_train = tf.convert_to_tensor(X_train_bi)\n",
    "\n",
    "# test sets\n",
    "X_atac_test = tf.convert_to_tensor(X_test_atac)\n",
    "X_rna_test = tf.convert_to_tensor(X_test_rna)\n",
    "X_bimodal_test = tf.convert_to_tensor(X_test_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Try plotting with UMAP (later)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Try plotting with UMAP (later)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define architecture '''\n",
    "HIDDEN_LAYER_DIMS = [500, 500, 2000]\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "def nn_model(n_features, dropout_rate, l2_lambda):\n",
    "    \"\"\"\n",
    "    Returns a tf.keras.Model for cell-type classification with the specifications \n",
    "    listed above.\n",
    "    \n",
    "    Arguments:\n",
    "      n_features: the number of features of the datapoints used\n",
    "        as input to the model (used to determine the input shape)\n",
    "      dropout_rate: the dropout rate of the dropout layers\n",
    "      l2_lambda: the weight of the L2 regularization penalty on the\n",
    "        weights (but not the biases) of the model\n",
    "    \n",
    "    Returns:\n",
    "      model: a tf.keras.Model for tSNE with the specifications\n",
    "        listed above\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    \n",
    "    \n",
    "    l2_reg = tf.keras.regularizers.l2(l=l2_lambda)\n",
    "    ######################## BEGIN YOUR ANSWER ########################\n",
    "    model.add(tf.keras.layers.Dense(HIDDEN_LAYER_DIMS[0], batch_input_shape=(None, n_features), \n",
    "                                    activation='relu', kernel_regularizer=l2_reg))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(HIDDEN_LAYER_DIMS[1], batch_input_shape=(None, n_features),\n",
    "                                   activation='relu', kernel_regularizer=l2_reg))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(HIDDEN_LAYER_DIMS[2], batch_input_shape=(None, n_features),\n",
    "                                   activation='relu', kernel_regularizer=l2_reg))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(OUTPUT_DIM, batch_input_shape=(None, n_features),\n",
    "                                   activation=tf.nn.softmax, kernel_regularizer=l2_reg))\n",
    "    \n",
    "    ######################### END YOUR ANSWER #########################\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 500)               300500    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 6003      \n",
      "=================================================================\n",
      "Total params: 1,559,003\n",
      "Trainable params: 1,559,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Give summary of architecture '''\n",
    "model = nn_model(n_features, dropout_rate=dropout_rate, l2_lambda=l2_lambda)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_shuffle_data(arrays, n_samples=None):\n",
    "    \"\"\"subsamples examples from a list of datasets\n",
    "    \n",
    "    samples `n_samples` without replacement from along the first dimension\n",
    "    of each array in `arrays`. The same first-dimension slices are \n",
    "    selected for each array in `arrays`.\n",
    "    \n",
    "    Arguments:\n",
    "      arrays: the arrays to be sliced, all must have the same size along \n",
    "        their first dimension.\n",
    "      n_samples: (None) the number of samples to be selected, `n_samples` must \n",
    "        be less than or equal to the length of the arrays. If n_samples it not\n",
    "        passed or is `None`. Then each array in `arrays` will be \n",
    "        shuffled in the same way and returned.\n",
    "      \n",
    "    Returns:\n",
    "      sampled: a `tuple` of len the same as `len(arrays)` where each\n",
    "        element is an array of len `n_samples`\n",
    "    \"\"\"\n",
    "    batch_len = arrays[0].shape[0]\n",
    "    n_samples = batch_len if n_samples is None else n_samples\n",
    "\n",
    "    err_msg = 'all arrays must have the same size along their first dimension'\n",
    "    assert all(batch_len == x.shape[0] for x in arrays), err_msg \n",
    "    err_msg = 'n cannot be greater then the length of the arrays'\n",
    "    assert n_samples <= batch_len, err_msg\n",
    "\n",
    "    sampling_idxs = tf.random.shuffle(tf.range(batch_len))[:n_samples]\n",
    "    sampled = tuple(tf.gather(x, sampling_idxs, axis=0) for x in arrays)\n",
    "    \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train step '''\n",
    "def train_step(model, loss, optimizer, x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Performs one training step on a model given a loss, optimizer, inputs,\n",
    "    and labels.\n",
    "    \n",
    "    Arguments:\n",
    "      model: the model on which the pass will be performed\n",
    "      loss: the loss function to be evaluated, from which the gradients will be\n",
    "        computed\n",
    "      optimizer: a `tf.optimizers` object defining the optimization scheme\n",
    "      x_batch: model training inputs\n",
    "      y_batch: model training labels\n",
    "      \n",
    "    Returns:\n",
    "      loss_value: the computed loss for the forward training pass\n",
    "    \"\"\"\n",
    "#     print('X_batch : {} \\n y_batch : {}'.format(x_batch, y_batch))\n",
    "    with tf.GradientTape() as tape:\n",
    "#         print(f'x_batch: {x_batch}')\n",
    "#         print(f'y_batch: {y_batch}')\n",
    "\n",
    "        y_batch_pred = model(x_batch, training=True)\n",
    "        loss_value = loss(y_batch, y_batch_pred)\n",
    "#         print(f\"model losses type: {model.losses}\")\n",
    "        loss_value += sum(model.losses)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training Loop '''\n",
    "def training(x_train, y_train, x_val, y_val, hyperparam_config, num_epochs, batch_size,\n",
    "             save_model=True, model_dir='models/best_model'):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Train a fully-connected network to classify rna_seq + atac-seq by cell type\n",
    "    \n",
    "    Arguments:\n",
    "      x_train: input training set\n",
    "      y_train: label training set\n",
    "      x_val: input validation set\n",
    "      y_val: label validation set\n",
    "      hyperparam_config: a dictionary that stores a hyperparameter configuration,\n",
    "                         including:\n",
    "                           - \"dropout_rate\": dropout rate (1 - keep probability),\n",
    "                           - \"l2\": coefficient lambda for L2 regularization,\n",
    "                           - \"lr\": learning rate for RMSProp optimizer\n",
    "      num_epochs: number of epochs to train\n",
    "      batch_size: training mini-batch size (must be same as the batch size for pairwise P calculation)\n",
    "      save_model: whether or not to save the best model based on the validation loss\n",
    "      model_dir: location where model will be saved\n",
    "    \n",
    "    Returns:\n",
    "      best_loss: best validation loss\n",
    "      best_kl_div: validation KL loss from the epoch that has best validation loss\n",
    "    \"\"\"\n",
    "    \n",
    "    ################################################################################\n",
    "    # Make sure to wrap train_step with tf.function to speed up training!\n",
    "    train_fn = tf.function(train_step)\n",
    "#     train_fn = train_step\n",
    "    ################################################################################ \n",
    "    \n",
    "    ######################## BEGIN YOUR ANSWER ########################\n",
    "    # get num_batches\n",
    "    num_samples, num_features = x_train.shape\n",
    "    num_batches = np.floor(num_samples) // batch_size\n",
    "    \n",
    "    # initalize model, loss, and optimizers\n",
    "    dropout_rate = hyperparam_config['dropout_rate']\n",
    "    l2_lambda = hyperparam_config['l2_lambda']\n",
    "    lr = hyperparam_config['lr']\n",
    "    model = nn_model(n_features=num_features, dropout_rate=dropout_rate, l2_lambda=l2_lambda)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy() # todo - make sure this is correct\n",
    "    optimizer = K.optimizers.SGD(learning_rate=lr)  #TODO - double check this optimizer\n",
    "\n",
    "    \n",
    "    # Get initial loss for comparison \n",
    "    print('getting initial loss')\n",
    "    best_loss = loss_fn(y_val, model(x_val))\n",
    "    \n",
    "    # init progress bars\n",
    "    epoch_pbar = tqdm(total=num_epochs, desc=\"Training Epochs\")\n",
    "    batch_pbar = tqdm(desc=\"Training Steps\")\n",
    "    \n",
    "    \n",
    "    n = x_train.shape[0]\n",
    "    # for each epoch   \n",
    "    # start training loop \n",
    "    for epoch in range(num_epochs):\n",
    "        # shuffle data\n",
    "        if epoch >= 1: x_train, y_train = sample_shuffle_data([x_train, y_train], num_samples)\n",
    "        \n",
    "        batch_pbar.reset(num_batches)\n",
    "        for step in range(int(num_batches)):\n",
    "            # getting indices of batches to train on\n",
    "            range_begin = (step * batch_size) % (x_train.shape[0] - batch_size) #taking mod to prevent ix errors\n",
    "            range_end = range_begin + batch_size\n",
    "            batch_x = x_train[range_begin:range_end, :]\n",
    "            batch_y = y_train[range_begin:range_end, :]\n",
    "            epoch_loss = train_fn(model, loss_fn, optimizer,\n",
    "                                 batch_x, batch_y)\n",
    "  \n",
    "            \n",
    "            batch_pbar.update()\n",
    "    \n",
    "        # compute and print loss on validation data\n",
    "        val_loss = loss_fn(y_val, model(x_val)) #note - don't need reg_coeff defined because already defined using functools.partial\n",
    "\n",
    "        tf.print(\"epoch: {:02d}, loss: {:5.3f}\".format(epoch + 1, val_loss))\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            if save_model:\n",
    "                # if directory hasn't been created, create it\n",
    "                if not os.path.isdir('models'):\n",
    "                    !mkdir -p models\n",
    "                # if model has already been saved, remove folder and save again\n",
    "                if os.path.isdir(model_dir):\n",
    "                    shutil.rmtree(model_dir)\n",
    "                # make directory again and save\n",
    "#                 !mkdir -p models/best_loss\n",
    "                model.save(model_dir)\n",
    "        batch_pbar.refresh()\n",
    "        epoch_pbar.update()\n",
    "            \n",
    "                \n",
    "    ######################### END YOUR ANSWER #########################\n",
    "    \n",
    "    \n",
    "    return best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get useful parameters for defining architecture '''\n",
    "X_shape = tf.shape(X_bimodal_train)\n",
    "assert len(X_shape) == 2, f'Order of input tensor shoud be 2 but is {len(X_shape)}'\n",
    "num_cells, num_features = X_shape\n",
    "dropout_rate = 0.1\n",
    "l2_lambda = 1e-6\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(426, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_atac_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_72 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5b37be2ac04486a93fc9f957d8c5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=50, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e546412939574705bbc1f448994d9643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.121\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 02, loss: 1.118\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 03, loss: 1.116\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 04, loss: 1.116\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 05, loss: 1.113\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 06, loss: 1.112\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 07, loss: 1.112\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 08, loss: 1.111\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 09, loss: 1.111\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 10, loss: 1.110\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 11, loss: 1.110\n",
      "INFO:tensorflow:Assets written to: models/atac_only_model_50_epochs/assets\n",
      "epoch: 12, loss: 1.112\n",
      "epoch: 13, loss: 1.113\n",
      "epoch: 14, loss: 1.111\n",
      "epoch: 15, loss: 1.112\n",
      "epoch: 16, loss: 1.113\n",
      "epoch: 17, loss: 1.114\n",
      "epoch: 18, loss: 1.115\n",
      "epoch: 19, loss: 1.117\n",
      "epoch: 20, loss: 1.118\n",
      "epoch: 21, loss: 1.119\n",
      "epoch: 22, loss: 1.121\n",
      "epoch: 23, loss: 1.122\n",
      "epoch: 24, loss: 1.124\n",
      "epoch: 25, loss: 1.125\n",
      "epoch: 26, loss: 1.127\n",
      "epoch: 27, loss: 1.129\n",
      "epoch: 28, loss: 1.131\n",
      "epoch: 29, loss: 1.134\n",
      "epoch: 30, loss: 1.135\n",
      "epoch: 31, loss: 1.138\n",
      "epoch: 32, loss: 1.140\n",
      "epoch: 33, loss: 1.142\n",
      "epoch: 34, loss: 1.144\n",
      "epoch: 35, loss: 1.146\n",
      "epoch: 36, loss: 1.148\n",
      "epoch: 37, loss: 1.151\n",
      "epoch: 38, loss: 1.154\n",
      "epoch: 39, loss: 1.156\n",
      "epoch: 40, loss: 1.158\n",
      "epoch: 41, loss: 1.161\n",
      "epoch: 42, loss: 1.164\n",
      "epoch: 43, loss: 1.166\n",
      "epoch: 44, loss: 1.168\n",
      "epoch: 45, loss: 1.171\n",
      "epoch: 46, loss: 1.173\n",
      "epoch: 47, loss: 1.175\n",
      "epoch: 48, loss: 1.177\n",
      "epoch: 49, loss: 1.180\n",
      "epoch: 50, loss: 1.182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.1101297>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Train model using ATAC-seq as features'''\n",
    "# define hyperparameters\n",
    "hyperparams_atac = {'dropout_rate':0.1, 'lr': 0.001, 'l2_lambda': l2_lambda}\n",
    "\n",
    "\n",
    "training(X_atac_train, Y_atac_train, X_atac_test, Y_atac_test, \n",
    "         hyperparam_config=hyperparams_atac,\n",
    "        num_epochs=n_epochs,\n",
    "        batch_size=100,\n",
    "        save_model=True,\n",
    "        model_dir=f'models/atac_only_model_{n_epochs}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_76 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7304cab35abb4244b7187374d778c44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=50, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c60c91b07d54c0eb45fe79c754759d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.116\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 02, loss: 1.109\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 03, loss: 1.104\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 04, loss: 1.097\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 05, loss: 1.095\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 06, loss: 1.089\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 07, loss: 1.085\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 08, loss: 1.078\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 09, loss: 1.074\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 10, loss: 1.068\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 11, loss: 1.064\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 12, loss: 1.057\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 13, loss: 1.051\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 14, loss: 1.043\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 15, loss: 1.038\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 16, loss: 1.030\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 17, loss: 1.024\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 18, loss: 1.016\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 19, loss: 1.008\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 20, loss: 1.001\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 21, loss: 0.994\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 22, loss: 0.987\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 23, loss: 0.978\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 24, loss: 0.970\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 25, loss: 0.963\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 26, loss: 0.954\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 27, loss: 0.947\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 28, loss: 0.940\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 29, loss: 0.931\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 30, loss: 0.926\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 31, loss: 0.918\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 32, loss: 0.912\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 33, loss: 0.905\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 34, loss: 0.899\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 35, loss: 0.892\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 36, loss: 0.888\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 37, loss: 0.881\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 38, loss: 0.876\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 39, loss: 0.871\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 40, loss: 0.866\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 41, loss: 0.861\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 42, loss: 0.857\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 43, loss: 0.852\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 44, loss: 0.849\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 45, loss: 0.845\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 46, loss: 0.841\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 47, loss: 0.837\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 48, loss: 0.835\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 49, loss: 0.832\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n",
      "epoch: 50, loss: 0.829\n",
      "INFO:tensorflow:Assets written to: models/rna_only_model_50_epochs/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8293294>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Train model using RNA-seq as features'''\n",
    "# define hyperparameters\n",
    "hyperparams_rna = {'dropout_rate':0.1, 'lr': 0.001, 'l2_lambda': l2_lambda}\n",
    "\n",
    "training(X_rna_train, Y_rna_train, X_rna_test, Y_rna_test, \n",
    "         hyperparam_config=hyperparams_rna,\n",
    "        num_epochs=50,\n",
    "        batch_size=100,\n",
    "        save_model=True,\n",
    "        model_dir=f'models/rna_only_model_{n_epochs}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_80 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ffbec18a574a5fb2a703c533a88b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=50, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ce240b602b4a7d867a1a1b8fef550c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.102\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 02, loss: 1.094\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 03, loss: 1.089\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 04, loss: 1.083\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 05, loss: 1.078\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 06, loss: 1.073\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 07, loss: 1.069\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 08, loss: 1.065\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 09, loss: 1.062\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 10, loss: 1.058\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 11, loss: 1.053\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 12, loss: 1.051\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 13, loss: 1.046\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 14, loss: 1.043\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 15, loss: 1.040\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 16, loss: 1.037\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 17, loss: 1.034\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 18, loss: 1.031\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 19, loss: 1.029\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 20, loss: 1.026\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 21, loss: 1.023\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 22, loss: 1.021\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 23, loss: 1.019\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 24, loss: 1.017\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 25, loss: 1.015\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 26, loss: 1.013\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 27, loss: 1.011\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 28, loss: 1.009\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 29, loss: 1.008\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 30, loss: 1.007\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 31, loss: 1.006\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 32, loss: 1.004\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 33, loss: 1.003\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 34, loss: 1.002\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 35, loss: 1.002\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 36, loss: 1.001\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 37, loss: 1.000\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 38, loss: 0.999\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 39, loss: 0.998\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 40, loss: 0.998\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 41, loss: 0.997\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 42, loss: 0.996\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 43, loss: 0.996\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 44, loss: 0.995\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 45, loss: 0.994\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 46, loss: 0.994\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 47, loss: 0.993\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 48, loss: 0.993\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 49, loss: 0.992\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n",
      "epoch: 50, loss: 0.992\n",
      "INFO:tensorflow:Assets written to: models/bimodal_model_50_epochs/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9919035>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Train model using both ATAC and RNA as features'''\n",
    "# define hyperparameters\n",
    "hyperparams_bimodal = {'dropout_rate':0.1, 'lr': 0.001, 'l2_lambda': l2_lambda}\n",
    "\n",
    "training(X_bimodal_train, Y_bimodal_train, X_bimodal_test, Y_bimodal_test, \n",
    "         hyperparam_config=hyperparams_bimodal,\n",
    "        num_epochs=50,\n",
    "        batch_size=100,\n",
    "        save_model=True,\n",
    "        model_dir=f'models/bimodal_model_{n_epochs}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Hyperparameter search using validation set'''\n",
    "def grid_search(x_train, y_train, dropout_rates, l2_lambdas, learning_rates, num_epochs=40, batch_size=300):\n",
    "    \"\"\"\n",
    "    Perform grid search for the best tSNE hyperparameters\n",
    "    \n",
    "    Arguments:\n",
    "      x_train: input training set\n",
    "      y_train: label training set\n",
    "      dropout_rates: dropout rates to try\n",
    "      l2_lambdas: L2 lambda coefficients to try\n",
    "      learning_rates: learning rates to try\n",
    "      num_epochs: number of epochs to train\n",
    "      batch_size: training mini-batch size\n",
    "    \n",
    "    Returns:\n",
    "      losses: list losses for configurations tested where\n",
    "        losses[i] = [dropout_rate, l2_lambda, learning_rate, best_loss, best_kl_divgergence]\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(total = len(dropout_rates) * len(l2_lambdas) * len(learning_rates))\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for l2_lambda in l2_lambdas:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(\"training with dropout:{} l2:{} lr:{}\".format(dropout_rate,l2_lambda,learning_rate))\n",
    "                # DO NOT shuffle your validation/train set because the pairwise label are calculated by batch\n",
    "                # Use the last batch in train set as the validation set\n",
    "                subset_x_train, subset_y_train=(x_train[0:-batch_size],y_train[0:-batch_size])\n",
    "                subset_x_val, subset_y_val = (x_train[-batch_size:],y_train[-batch_size:])\n",
    "                hyperparam_config = {'dropout_rate': dropout_rate,\n",
    "                                     'l2_lambda': l2_lambda,\n",
    "                                     'lr': learning_rate}\n",
    "\n",
    "                best_loss = training(subset_x_train, subset_y_train,\n",
    "                                     subset_x_val, subset_y_val,\n",
    "                                     hyperparam_config,\n",
    "                                     num_epochs,\n",
    "                                     batch_size,\n",
    "                                     save_model=False)\n",
    "\n",
    "                losses.append([dropout_rate, l2_lambda, learning_rate, best_loss])\n",
    "                pbar.update(1)\n",
    "    pbar.close()\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.5, 0.2, 0.1]\n",
    "l2_lambdas = [1e-03, 1e-06, 0]\n",
    "learning_rates = [0.1, 0.01, 1e-3, 1e-4]\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7255022091424daf6fb475a0e16a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dropout:0.5 l2:0.001 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_84 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1102455fd92543a2b65ea816fa920fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797d4b0753064a189b5df4f203c1048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.104\n",
      "epoch: 02, loss: 1.094\n",
      "epoch: 03, loss: 1.067\n",
      "epoch: 04, loss: 1.053\n",
      "epoch: 05, loss: 1.015\n",
      "epoch: 06, loss: 1.002\n",
      "epoch: 07, loss: 0.965\n",
      "epoch: 08, loss: 0.964\n",
      "epoch: 09, loss: 0.933\n",
      "epoch: 10, loss: 0.953\n",
      "training with dropout:0.5 l2:0.001 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_88 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d884eb8d2514ad091cc89a84e99a73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c61061e9f441e399920b5502d0c6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.124\n",
      "epoch: 02, loss: 1.117\n",
      "epoch: 03, loss: 1.108\n",
      "epoch: 04, loss: 1.106\n",
      "epoch: 05, loss: 1.108\n",
      "epoch: 06, loss: 1.102\n",
      "epoch: 07, loss: 1.099\n",
      "epoch: 08, loss: 1.097\n",
      "epoch: 09, loss: 1.094\n",
      "epoch: 10, loss: 1.088\n",
      "training with dropout:0.5 l2:0.001 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_92 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785ab12231fd401ba60002b74e5b3b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56483d016814ac2b1d5975df69e967d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.130\n",
      "epoch: 02, loss: 1.134\n",
      "epoch: 03, loss: 1.137\n",
      "epoch: 04, loss: 1.139\n",
      "epoch: 05, loss: 1.137\n",
      "epoch: 06, loss: 1.138\n",
      "epoch: 07, loss: 1.139\n",
      "epoch: 08, loss: 1.139\n",
      "epoch: 09, loss: 1.136\n",
      "epoch: 10, loss: 1.135\n",
      "training with dropout:0.5 l2:0.001 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_96 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaf20107ab641b9b399b7b292ea460e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9e4f4e60dc4e1fa17acd89e93e751a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.126\n",
      "epoch: 02, loss: 1.124\n",
      "epoch: 03, loss: 1.123\n",
      "epoch: 04, loss: 1.122\n",
      "epoch: 05, loss: 1.121\n",
      "epoch: 06, loss: 1.120\n",
      "epoch: 07, loss: 1.119\n",
      "epoch: 08, loss: 1.118\n",
      "epoch: 09, loss: 1.117\n",
      "epoch: 10, loss: 1.117\n",
      "training with dropout:0.5 l2:1e-06 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_100 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f90cd1270104c92aca7687d91ba9241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5f4735f7b54d21b1ff4fe6f36d1064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.110\n",
      "epoch: 02, loss: 1.112\n",
      "epoch: 03, loss: 1.153\n",
      "epoch: 04, loss: 1.047\n",
      "epoch: 05, loss: 1.048\n",
      "epoch: 06, loss: 1.009\n",
      "epoch: 07, loss: 0.974\n",
      "epoch: 08, loss: 0.967\n",
      "epoch: 09, loss: 0.965\n",
      "epoch: 10, loss: 0.969\n",
      "training with dropout:0.5 l2:1e-06 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_104 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cc76c962474b68ba4477caee31a045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612bdccab73244848cc3e5b3da433f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.136\n",
      "epoch: 02, loss: 1.126\n",
      "epoch: 03, loss: 1.124\n",
      "epoch: 04, loss: 1.123\n",
      "epoch: 05, loss: 1.111\n",
      "epoch: 06, loss: 1.112\n",
      "epoch: 07, loss: 1.108\n",
      "epoch: 08, loss: 1.106\n",
      "epoch: 09, loss: 1.103\n",
      "epoch: 10, loss: 1.103\n",
      "training with dropout:0.5 l2:1e-06 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_108 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98feec99a35843ac9d263e93210bf1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb935957040246b0ba1909f264dd9dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.139\n",
      "epoch: 02, loss: 1.135\n",
      "epoch: 03, loss: 1.139\n",
      "epoch: 04, loss: 1.139\n",
      "epoch: 05, loss: 1.139\n",
      "epoch: 06, loss: 1.138\n",
      "epoch: 07, loss: 1.138\n",
      "epoch: 08, loss: 1.138\n",
      "epoch: 09, loss: 1.135\n",
      "epoch: 10, loss: 1.132\n",
      "training with dropout:0.5 l2:1e-06 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_112 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd21fcf1a7ee49179c287b65faddb243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdc8808f93b4a3589406c5883599fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.121\n",
      "epoch: 02, loss: 1.120\n",
      "epoch: 03, loss: 1.120\n",
      "epoch: 04, loss: 1.119\n",
      "epoch: 05, loss: 1.119\n",
      "epoch: 06, loss: 1.119\n",
      "epoch: 07, loss: 1.118\n",
      "epoch: 08, loss: 1.118\n",
      "epoch: 09, loss: 1.118\n",
      "epoch: 10, loss: 1.117\n",
      "training with dropout:0.5 l2:0 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_116 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75eab6d19c1846e280395cc76110ef9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff16da80be34604add2a3e9c971ed81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.161\n",
      "epoch: 02, loss: 1.143\n",
      "epoch: 03, loss: 1.061\n",
      "epoch: 04, loss: 1.056\n",
      "epoch: 05, loss: 1.036\n",
      "epoch: 06, loss: 0.987\n",
      "epoch: 07, loss: 0.956\n",
      "epoch: 08, loss: 0.934\n",
      "epoch: 09, loss: 0.934\n",
      "epoch: 10, loss: 0.914\n",
      "training with dropout:0.5 l2:0 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_120 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d24e2201f146559bbed65108c4cef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c404faf1bfb425da21bb471485298ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.108\n",
      "epoch: 02, loss: 1.105\n",
      "epoch: 03, loss: 1.097\n",
      "epoch: 04, loss: 1.094\n",
      "epoch: 05, loss: 1.094\n",
      "epoch: 06, loss: 1.083\n",
      "epoch: 07, loss: 1.082\n",
      "epoch: 08, loss: 1.082\n",
      "epoch: 09, loss: 1.073\n",
      "epoch: 10, loss: 1.075\n",
      "training with dropout:0.5 l2:0 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_124 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e6e2183be643dd80dbe729ed95cbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ed197f00b54cf8881874bdace8bf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.153\n",
      "epoch: 02, loss: 1.126\n",
      "epoch: 03, loss: 1.121\n",
      "epoch: 04, loss: 1.121\n",
      "epoch: 05, loss: 1.121\n",
      "epoch: 06, loss: 1.120\n",
      "epoch: 07, loss: 1.120\n",
      "epoch: 08, loss: 1.118\n",
      "epoch: 09, loss: 1.117\n",
      "epoch: 10, loss: 1.117\n",
      "training with dropout:0.5 l2:0 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_128 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf18abb2f5f84611b0d085200495e649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dca4d251aef43ad92f1cb2a8f069cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.114\n",
      "epoch: 02, loss: 1.112\n",
      "epoch: 03, loss: 1.110\n",
      "epoch: 04, loss: 1.109\n",
      "epoch: 05, loss: 1.107\n",
      "epoch: 06, loss: 1.106\n",
      "epoch: 07, loss: 1.105\n",
      "epoch: 08, loss: 1.105\n",
      "epoch: 09, loss: 1.104\n",
      "epoch: 10, loss: 1.103\n",
      "training with dropout:0.2 l2:0.001 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_132 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a56f39c470946f5b7100ee5a3b0a7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e971faeb14c04a238aeb1e9f3b624708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.139\n",
      "epoch: 02, loss: 1.181\n",
      "epoch: 03, loss: 1.015\n",
      "epoch: 04, loss: 1.020\n",
      "epoch: 05, loss: 1.024\n",
      "epoch: 06, loss: 1.029\n",
      "epoch: 07, loss: 1.037\n",
      "epoch: 08, loss: 1.038\n",
      "epoch: 09, loss: 1.045\n",
      "epoch: 10, loss: 1.049\n",
      "training with dropout:0.2 l2:0.001 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_136 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c9cc65fdfd486f82d67eff47d51f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09f37eee4e2467f8b555f2877ff01f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.113\n",
      "epoch: 02, loss: 1.109\n",
      "epoch: 03, loss: 1.105\n",
      "epoch: 04, loss: 1.105\n",
      "epoch: 05, loss: 1.103\n",
      "epoch: 06, loss: 1.098\n",
      "epoch: 07, loss: 1.107\n",
      "epoch: 08, loss: 1.100\n",
      "epoch: 09, loss: 1.102\n",
      "epoch: 10, loss: 1.100\n",
      "training with dropout:0.2 l2:0.001 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_140 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863c462260a34b9dbf47bdccf384c3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdca03cc196414ab612313bd0aaa6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.129\n",
      "epoch: 02, loss: 1.115\n",
      "epoch: 03, loss: 1.114\n",
      "epoch: 04, loss: 1.114\n",
      "epoch: 05, loss: 1.115\n",
      "epoch: 06, loss: 1.115\n",
      "epoch: 07, loss: 1.116\n",
      "epoch: 08, loss: 1.116\n",
      "epoch: 09, loss: 1.116\n",
      "epoch: 10, loss: 1.115\n",
      "training with dropout:0.2 l2:0.001 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_144 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e816103a56474bc8adf700a6c4228fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742b25407818444dafed3eeb954393dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.176\n",
      "epoch: 02, loss: 1.171\n",
      "epoch: 03, loss: 1.167\n",
      "epoch: 04, loss: 1.163\n",
      "epoch: 05, loss: 1.159\n",
      "epoch: 06, loss: 1.156\n",
      "epoch: 07, loss: 1.153\n",
      "epoch: 08, loss: 1.151\n",
      "epoch: 09, loss: 1.148\n",
      "epoch: 10, loss: 1.147\n",
      "training with dropout:0.2 l2:1e-06 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_148 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fbf8590a1e4dbf813d589ae92e325c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfcdeda476e4db1a06f5457c75cf879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.126\n",
      "epoch: 02, loss: 1.105\n",
      "epoch: 03, loss: 1.026\n",
      "epoch: 04, loss: 1.027\n",
      "epoch: 05, loss: 1.043\n",
      "epoch: 06, loss: 1.061\n",
      "epoch: 07, loss: 1.076\n",
      "epoch: 08, loss: 1.081\n",
      "epoch: 09, loss: 1.088\n",
      "epoch: 10, loss: 1.091\n",
      "training with dropout:0.2 l2:1e-06 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_152 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2a0fd92b9c48fc90cf8aebcd892ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def19315b8d94eb086cb78ccc460f781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.110\n",
      "epoch: 02, loss: 1.110\n",
      "epoch: 03, loss: 1.105\n",
      "epoch: 04, loss: 1.105\n",
      "epoch: 05, loss: 1.101\n",
      "epoch: 06, loss: 1.109\n",
      "epoch: 07, loss: 1.097\n",
      "epoch: 08, loss: 1.102\n",
      "epoch: 09, loss: 1.108\n",
      "epoch: 10, loss: 1.106\n",
      "training with dropout:0.2 l2:1e-06 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_156 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81b12ed40f2433687f86849e5831208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73fe81d83ef4f1b85dcd236e02b2dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.179\n",
      "epoch: 02, loss: 1.162\n",
      "epoch: 03, loss: 1.155\n",
      "epoch: 04, loss: 1.151\n",
      "epoch: 05, loss: 1.150\n",
      "epoch: 06, loss: 1.149\n",
      "epoch: 07, loss: 1.148\n",
      "epoch: 08, loss: 1.147\n",
      "epoch: 09, loss: 1.146\n",
      "epoch: 10, loss: 1.144\n",
      "training with dropout:0.2 l2:1e-06 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_160 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f561775f3d4562a49c5584792cbc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b3e2f84d40451dbe6f39744c87aeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.147\n",
      "epoch: 02, loss: 1.145\n",
      "epoch: 03, loss: 1.144\n",
      "epoch: 04, loss: 1.142\n",
      "epoch: 05, loss: 1.141\n",
      "epoch: 06, loss: 1.141\n",
      "epoch: 07, loss: 1.140\n",
      "epoch: 08, loss: 1.139\n",
      "epoch: 09, loss: 1.139\n",
      "epoch: 10, loss: 1.139\n",
      "training with dropout:0.2 l2:0 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_164 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e688b96b5f7412090e7a7e44c7afc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efea673545ea405a9cba8388cdfb337a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.082\n",
      "epoch: 02, loss: 1.056\n",
      "epoch: 03, loss: 1.048\n",
      "epoch: 04, loss: 1.042\n",
      "epoch: 05, loss: 1.045\n",
      "epoch: 06, loss: 1.061\n",
      "epoch: 07, loss: 1.075\n",
      "epoch: 08, loss: 1.083\n",
      "epoch: 09, loss: 1.078\n",
      "epoch: 10, loss: 1.090\n",
      "training with dropout:0.2 l2:0 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_168 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1595102e601d49b48b5f5d90fc461dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebeceeed45d4c2a8acf620a6f522208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.138\n",
      "epoch: 02, loss: 1.137\n",
      "epoch: 03, loss: 1.134\n",
      "epoch: 04, loss: 1.130\n",
      "epoch: 05, loss: 1.135\n",
      "epoch: 06, loss: 1.130\n",
      "epoch: 07, loss: 1.136\n",
      "epoch: 08, loss: 1.142\n",
      "epoch: 09, loss: 1.145\n",
      "epoch: 10, loss: 1.151\n",
      "training with dropout:0.2 l2:0 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_172 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2b9e57f4de42768d7884b3f955afd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7876af9a948f4b4099436d5da520c594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.193\n",
      "epoch: 02, loss: 1.168\n",
      "epoch: 03, loss: 1.158\n",
      "epoch: 04, loss: 1.154\n",
      "epoch: 05, loss: 1.152\n",
      "epoch: 06, loss: 1.151\n",
      "epoch: 07, loss: 1.150\n",
      "epoch: 08, loss: 1.148\n",
      "epoch: 09, loss: 1.149\n",
      "epoch: 10, loss: 1.148\n",
      "training with dropout:0.2 l2:0 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_176 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a045961a9c2447ef84b915610b6bf8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fd926eab2043ed8c93dcabf34458d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.138\n",
      "epoch: 02, loss: 1.137\n",
      "epoch: 03, loss: 1.136\n",
      "epoch: 04, loss: 1.135\n",
      "epoch: 05, loss: 1.134\n",
      "epoch: 06, loss: 1.133\n",
      "epoch: 07, loss: 1.133\n",
      "epoch: 08, loss: 1.132\n",
      "epoch: 09, loss: 1.131\n",
      "epoch: 10, loss: 1.131\n",
      "training with dropout:0.1 l2:0.001 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_180 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157dd1cd0bc84dc8a52ea2d35d8eaee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7402349f8f4b69831cddc2413875fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.111\n",
      "epoch: 02, loss: 1.051\n",
      "epoch: 03, loss: 1.059\n",
      "epoch: 04, loss: 1.034\n",
      "epoch: 05, loss: 1.051\n",
      "epoch: 06, loss: 1.052\n",
      "epoch: 07, loss: 1.063\n",
      "epoch: 08, loss: 1.069\n",
      "epoch: 09, loss: 1.074\n",
      "epoch: 10, loss: 1.080\n",
      "training with dropout:0.1 l2:0.001 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_184 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d17b7e9e38b472187f38e68103e8ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c046253d389e432cba776b634857ba53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.122\n",
      "epoch: 02, loss: 1.115\n",
      "epoch: 03, loss: 1.118\n",
      "epoch: 04, loss: 1.125\n",
      "epoch: 05, loss: 1.115\n",
      "epoch: 06, loss: 1.114\n",
      "epoch: 07, loss: 1.118\n",
      "epoch: 08, loss: 1.115\n",
      "epoch: 09, loss: 1.119\n",
      "epoch: 10, loss: 1.122\n",
      "training with dropout:0.1 l2:0.001 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_188 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76e74c46a4a4cb28181eaa7ecd3a6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0584ac53fa584bbe9a7a6f3e66e7af3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.125\n",
      "epoch: 02, loss: 1.114\n",
      "epoch: 03, loss: 1.109\n",
      "epoch: 04, loss: 1.107\n",
      "epoch: 05, loss: 1.105\n",
      "epoch: 06, loss: 1.104\n",
      "epoch: 07, loss: 1.104\n",
      "epoch: 08, loss: 1.103\n",
      "epoch: 09, loss: 1.102\n",
      "epoch: 10, loss: 1.102\n",
      "training with dropout:0.1 l2:0.001 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d23e1b639104afd8128606fce5a5f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f29020de5644686bd9998bbb3c24d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.195\n",
      "epoch: 02, loss: 1.192\n",
      "epoch: 03, loss: 1.189\n",
      "epoch: 04, loss: 1.185\n",
      "epoch: 05, loss: 1.182\n",
      "epoch: 06, loss: 1.180\n",
      "epoch: 07, loss: 1.177\n",
      "epoch: 08, loss: 1.175\n",
      "epoch: 09, loss: 1.172\n",
      "epoch: 10, loss: 1.170\n",
      "training with dropout:0.1 l2:1e-06 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_196 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df9a711c0434fb9a8ed7df239680f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a901ed721db4ca4a85997656c91a403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.211\n",
      "epoch: 02, loss: 1.050\n",
      "epoch: 03, loss: 1.013\n",
      "epoch: 04, loss: 1.020\n",
      "epoch: 05, loss: 1.024\n",
      "epoch: 06, loss: 1.027\n",
      "epoch: 07, loss: 1.040\n",
      "epoch: 08, loss: 1.047\n",
      "epoch: 09, loss: 1.052\n",
      "epoch: 10, loss: 1.058\n",
      "training with dropout:0.1 l2:1e-06 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_200 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d86bd842d2b497d8529c838cbe0c3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe46bcd8d74c4f61b9c0b8172e76f3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.126\n",
      "epoch: 02, loss: 1.126\n",
      "epoch: 03, loss: 1.124\n",
      "epoch: 04, loss: 1.121\n",
      "epoch: 05, loss: 1.123\n",
      "epoch: 06, loss: 1.126\n",
      "epoch: 07, loss: 1.131\n",
      "epoch: 08, loss: 1.128\n",
      "epoch: 09, loss: 1.134\n",
      "epoch: 10, loss: 1.137\n",
      "training with dropout:0.1 l2:1e-06 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_204 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d385dd21764d3cbac4c03afd8ac550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2949fc42ea472fb870222285f1a277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.111\n",
      "epoch: 02, loss: 1.110\n",
      "epoch: 03, loss: 1.110\n",
      "epoch: 04, loss: 1.109\n",
      "epoch: 05, loss: 1.109\n",
      "epoch: 06, loss: 1.109\n",
      "epoch: 07, loss: 1.109\n",
      "epoch: 08, loss: 1.108\n",
      "epoch: 09, loss: 1.107\n",
      "epoch: 10, loss: 1.107\n",
      "training with dropout:0.1 l2:1e-06 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_208 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8936e5ba2948e39b96dd2794962066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1926bce046a4452ac835034a2c631ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.111\n",
      "epoch: 02, loss: 1.109\n",
      "epoch: 03, loss: 1.107\n",
      "epoch: 04, loss: 1.105\n",
      "epoch: 05, loss: 1.103\n",
      "epoch: 06, loss: 1.102\n",
      "epoch: 07, loss: 1.101\n",
      "epoch: 08, loss: 1.100\n",
      "epoch: 09, loss: 1.099\n",
      "epoch: 10, loss: 1.098\n",
      "training with dropout:0.1 l2:0 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_212 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314481115e0443d69d6447f1a8259581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f747da93a0ea424da36d5823c9ebdbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.087\n",
      "epoch: 02, loss: 1.068\n",
      "epoch: 03, loss: 1.057\n",
      "epoch: 04, loss: 1.058\n",
      "epoch: 05, loss: 1.077\n",
      "epoch: 06, loss: 1.092\n",
      "epoch: 07, loss: 1.107\n",
      "epoch: 08, loss: 1.119\n",
      "epoch: 09, loss: 1.128\n",
      "epoch: 10, loss: 1.139\n",
      "training with dropout:0.1 l2:0 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_216 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89b960c051646dea021f736fb3d1e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d8302a209545a8b06fa963f1a235a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.118\n",
      "epoch: 02, loss: 1.119\n",
      "epoch: 03, loss: 1.111\n",
      "epoch: 04, loss: 1.115\n",
      "epoch: 05, loss: 1.112\n",
      "epoch: 06, loss: 1.118\n",
      "epoch: 07, loss: 1.116\n",
      "epoch: 08, loss: 1.113\n",
      "epoch: 09, loss: 1.117\n",
      "epoch: 10, loss: 1.123\n",
      "training with dropout:0.1 l2:0 lr:0.001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_220 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45633b324875473483ad9459ad877283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a3a21a5ae8498b87ea106131c5fc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.135\n",
      "epoch: 02, loss: 1.128\n",
      "epoch: 03, loss: 1.125\n",
      "epoch: 04, loss: 1.123\n",
      "epoch: 05, loss: 1.121\n",
      "epoch: 06, loss: 1.120\n",
      "epoch: 07, loss: 1.119\n",
      "epoch: 08, loss: 1.118\n",
      "epoch: 09, loss: 1.118\n",
      "epoch: 10, loss: 1.117\n",
      "training with dropout:0.1 l2:0 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_224 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b903c7436d94a51870e8b026352589a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c9f7e462b043e9b12515c0a7500235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.221\n",
      "epoch: 02, loss: 1.214\n",
      "epoch: 03, loss: 1.209\n",
      "epoch: 04, loss: 1.203\n",
      "epoch: 05, loss: 1.199\n",
      "epoch: 06, loss: 1.194\n",
      "epoch: 07, loss: 1.190\n",
      "epoch: 08, loss: 1.186\n",
      "epoch: 09, loss: 1.182\n",
      "epoch: 10, loss: 1.179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' GRID-SEARCH for ATAC-seq as input'''\n",
    "atac_param_result = grid_search(X_atac_train, Y_atac_train, dropout_rates, l2_lambdas, learning_rates, NUM_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dae651ff864cc0820ba02d1a5970c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dropout:0.2 l2:0.001 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_256 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04beaf885fb4a2f8b057ae25fefdaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef47f8ac9724b32874f014573b8e600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.075\n",
      "epoch: 02, loss: 1.036\n",
      "epoch: 03, loss: 0.931\n",
      "epoch: 04, loss: 0.825\n",
      "epoch: 05, loss: 0.777\n",
      "epoch: 06, loss: 0.742\n",
      "epoch: 07, loss: 0.731\n",
      "epoch: 08, loss: 0.726\n",
      "epoch: 09, loss: 0.719\n",
      "epoch: 10, loss: 0.717\n",
      "training with dropout:0.2 l2:0.001 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_260 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d798044bfcf47dba1a9a93a2f31daa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cff23ab8a444febe6d98016bf4b1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.103\n",
      "epoch: 02, loss: 1.099\n",
      "epoch: 03, loss: 1.087\n",
      "epoch: 04, loss: 1.087\n",
      "epoch: 05, loss: 1.078\n",
      "epoch: 06, loss: 1.063\n",
      "epoch: 07, loss: 1.060\n",
      "epoch: 08, loss: 1.053\n",
      "epoch: 09, loss: 1.033\n",
      "epoch: 10, loss: 1.026\n",
      "training with dropout:0.2 l2:0.001 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_264 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb547bc43fdb41a1a7cdd087a3c16899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bfb50692f44a63b3e29354ac3c2019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.099\n",
      "epoch: 02, loss: 1.099\n",
      "epoch: 03, loss: 1.100\n",
      "epoch: 04, loss: 1.100\n",
      "epoch: 05, loss: 1.100\n",
      "epoch: 06, loss: 1.100\n",
      "epoch: 07, loss: 1.100\n",
      "epoch: 08, loss: 1.100\n",
      "epoch: 09, loss: 1.100\n",
      "epoch: 10, loss: 1.100\n",
      "training with dropout:0.2 l2:1e-06 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_268 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a637778a97e401db6df14ea646ed5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5764d43e8c8f403fba35d5117f29053c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.060\n",
      "epoch: 02, loss: 1.035\n",
      "epoch: 03, loss: 0.845\n",
      "epoch: 04, loss: 0.751\n",
      "epoch: 05, loss: 0.686\n",
      "epoch: 06, loss: 0.662\n",
      "epoch: 07, loss: 0.649\n",
      "epoch: 08, loss: 0.634\n",
      "epoch: 09, loss: 0.631\n",
      "epoch: 10, loss: 0.628\n",
      "training with dropout:0.2 l2:1e-06 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_272 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc3c2fd8c6547c7afd7ce606cbe99f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732099ec13be45eb950c9fcbf02714cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.088\n",
      "epoch: 02, loss: 1.074\n",
      "epoch: 03, loss: 1.070\n",
      "epoch: 04, loss: 1.058\n",
      "epoch: 05, loss: 1.047\n",
      "epoch: 06, loss: 1.034\n",
      "epoch: 07, loss: 1.019\n",
      "epoch: 08, loss: 1.004\n",
      "epoch: 09, loss: 0.988\n",
      "epoch: 10, loss: 0.976\n",
      "training with dropout:0.2 l2:1e-06 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_276 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa7aefd2c25499c9644f13fab1be538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6298fc1d44f4c9bbecc4e4fe8cb8931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.219\n",
      "epoch: 02, loss: 1.212\n",
      "epoch: 03, loss: 1.206\n",
      "epoch: 04, loss: 1.200\n",
      "epoch: 05, loss: 1.194\n",
      "epoch: 06, loss: 1.189\n",
      "epoch: 07, loss: 1.185\n",
      "epoch: 08, loss: 1.181\n",
      "epoch: 09, loss: 1.177\n",
      "epoch: 10, loss: 1.174\n",
      "training with dropout:0.1 l2:0.001 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_280 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b1a94859ad47e0a11ef8ed318ea904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26bf269302043dd971abbf9a2841856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.039\n",
      "epoch: 02, loss: 0.957\n",
      "epoch: 03, loss: 0.838\n",
      "epoch: 04, loss: 0.726\n",
      "epoch: 05, loss: 0.682\n",
      "epoch: 06, loss: 0.661\n",
      "epoch: 07, loss: 0.651\n",
      "epoch: 08, loss: 0.643\n",
      "epoch: 09, loss: 0.638\n",
      "epoch: 10, loss: 0.632\n",
      "training with dropout:0.1 l2:0.001 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_284 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939d99ac30ae4772a0af517bdc7c4dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338a5fdd18ce4710ac985dc74c3b4cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.113\n",
      "epoch: 02, loss: 1.102\n",
      "epoch: 03, loss: 1.091\n",
      "epoch: 04, loss: 1.083\n",
      "epoch: 05, loss: 1.070\n",
      "epoch: 06, loss: 1.059\n",
      "epoch: 07, loss: 1.053\n",
      "epoch: 08, loss: 1.033\n",
      "epoch: 09, loss: 1.021\n",
      "epoch: 10, loss: 1.005\n",
      "training with dropout:0.1 l2:0.001 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_288 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8214ea4bae2a4801863d05db967ff18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f8dfb64ad452aa2c614055de05666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.099\n",
      "epoch: 02, loss: 1.099\n",
      "epoch: 03, loss: 1.099\n",
      "epoch: 04, loss: 1.098\n",
      "epoch: 05, loss: 1.098\n",
      "epoch: 06, loss: 1.098\n",
      "epoch: 07, loss: 1.098\n",
      "epoch: 08, loss: 1.097\n",
      "epoch: 09, loss: 1.097\n",
      "epoch: 10, loss: 1.097\n",
      "training with dropout:0.1 l2:1e-06 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_292 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af935069277143199370b2a3bae686e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78662833988486e925dcbd9617a3869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.065\n",
      "epoch: 02, loss: 1.012\n",
      "epoch: 03, loss: 0.923\n",
      "epoch: 04, loss: 0.813\n",
      "epoch: 05, loss: 0.768\n",
      "epoch: 06, loss: 0.744\n",
      "epoch: 07, loss: 0.734\n",
      "epoch: 08, loss: 0.727\n",
      "epoch: 09, loss: 0.721\n",
      "epoch: 10, loss: 0.719\n",
      "training with dropout:0.1 l2:1e-06 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_296 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d836eaba9a347478cea1219680e24f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ca437459f34299b98ce157b0ca95a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.121\n",
      "epoch: 02, loss: 1.116\n",
      "epoch: 03, loss: 1.104\n",
      "epoch: 04, loss: 1.091\n",
      "epoch: 05, loss: 1.084\n",
      "epoch: 06, loss: 1.067\n",
      "epoch: 07, loss: 1.059\n",
      "epoch: 08, loss: 1.043\n",
      "epoch: 09, loss: 1.030\n",
      "epoch: 10, loss: 1.018\n",
      "training with dropout:0.1 l2:1e-06 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_300 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1d50c1526543d284c3b921d56854e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ed7d24e7574ec5a6b544e963f75241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.197\n",
      "epoch: 02, loss: 1.188\n",
      "epoch: 03, loss: 1.180\n",
      "epoch: 04, loss: 1.172\n",
      "epoch: 05, loss: 1.166\n",
      "epoch: 06, loss: 1.160\n",
      "epoch: 07, loss: 1.155\n",
      "epoch: 08, loss: 1.150\n",
      "epoch: 09, loss: 1.146\n",
      "epoch: 10, loss: 1.142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' GRID-SEARCH with RNA-seq as input'''\n",
    "dropout_rates = [0.2, 0.1]\n",
    "l2_lambdas = [1e-03, 1e-06]\n",
    "learning_rates = [0.1, 0.01, 1e-4]\n",
    "rna_param_result = grid_search(X_rna_train, Y_rna_train, dropout_rates, l2_lambdas, learning_rates, NUM_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95592790b6042bea92d74711422854a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dropout:0.2 l2:0.001 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_304 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56550b81a83e48fca361b5536b095a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13203cc561ba4dc1bcfa5879c43a80ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.064\n",
      "epoch: 02, loss: 1.037\n",
      "epoch: 03, loss: 0.823\n",
      "epoch: 04, loss: 0.807\n",
      "epoch: 05, loss: 0.796\n",
      "epoch: 06, loss: 0.788\n",
      "epoch: 07, loss: 0.797\n",
      "epoch: 08, loss: 0.790\n",
      "epoch: 09, loss: 0.784\n",
      "epoch: 10, loss: 0.785\n",
      "training with dropout:0.2 l2:0.001 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_308 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72de9e864f94e11828f59219a0c526e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7406fa587549c8a69af5babc1ad7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.092\n",
      "epoch: 02, loss: 1.062\n",
      "epoch: 03, loss: 1.046\n",
      "epoch: 04, loss: 1.043\n",
      "epoch: 05, loss: 1.025\n",
      "epoch: 06, loss: 1.014\n",
      "epoch: 07, loss: 1.005\n",
      "epoch: 08, loss: 0.998\n",
      "epoch: 09, loss: 0.989\n",
      "epoch: 10, loss: 0.982\n",
      "training with dropout:0.2 l2:0.001 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_312 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaed2a5d8024e18a8266b98686876b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcec4f404e148ce843e555436f7a456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.139\n",
      "epoch: 02, loss: 1.135\n",
      "epoch: 03, loss: 1.131\n",
      "epoch: 04, loss: 1.128\n",
      "epoch: 05, loss: 1.125\n",
      "epoch: 06, loss: 1.122\n",
      "epoch: 07, loss: 1.120\n",
      "epoch: 08, loss: 1.118\n",
      "epoch: 09, loss: 1.116\n",
      "epoch: 10, loss: 1.114\n",
      "training with dropout:0.2 l2:1e-06 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_316 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a09129b443945bb809059eadcf2c02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26caccdc13c44dcabf2f9042652a6684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.122\n",
      "epoch: 02, loss: 0.903\n",
      "epoch: 03, loss: 0.806\n",
      "epoch: 04, loss: 0.767\n",
      "epoch: 05, loss: 0.758\n",
      "epoch: 06, loss: 0.750\n",
      "epoch: 07, loss: 0.747\n",
      "epoch: 08, loss: 0.745\n",
      "epoch: 09, loss: 0.742\n",
      "epoch: 10, loss: 0.740\n",
      "training with dropout:0.2 l2:1e-06 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_320 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b18cae30fd74d7f86ffeaec1c3238d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c3776be1374445a8e6ff18283b31c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.086\n",
      "epoch: 02, loss: 1.063\n",
      "epoch: 03, loss: 1.048\n",
      "epoch: 04, loss: 1.041\n",
      "epoch: 05, loss: 1.027\n",
      "epoch: 06, loss: 1.018\n",
      "epoch: 07, loss: 1.011\n",
      "epoch: 08, loss: 1.004\n",
      "epoch: 09, loss: 0.998\n",
      "epoch: 10, loss: 0.994\n",
      "training with dropout:0.2 l2:1e-06 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_324 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18031ab042e44b6ab903f40a5eb10fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33620261b01d4545bf81e2a0e287ccc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.103\n",
      "epoch: 02, loss: 1.102\n",
      "epoch: 03, loss: 1.101\n",
      "epoch: 04, loss: 1.101\n",
      "epoch: 05, loss: 1.100\n",
      "epoch: 06, loss: 1.099\n",
      "epoch: 07, loss: 1.099\n",
      "epoch: 08, loss: 1.098\n",
      "epoch: 09, loss: 1.098\n",
      "epoch: 10, loss: 1.097\n",
      "training with dropout:0.1 l2:0.001 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_328 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6f80296c074dc2aab0038fae9b53cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d775f0775434f02b10276594526d194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.025\n",
      "epoch: 02, loss: 0.966\n",
      "epoch: 03, loss: 0.873\n",
      "epoch: 04, loss: 0.864\n",
      "epoch: 05, loss: 0.860\n",
      "epoch: 06, loss: 0.858\n",
      "epoch: 07, loss: 0.856\n",
      "epoch: 08, loss: 0.854\n",
      "epoch: 09, loss: 0.852\n",
      "epoch: 10, loss: 0.853\n",
      "training with dropout:0.1 l2:0.001 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_332 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1da5be6385a4212b3f41f156a1898d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b991ba349e24deda7ddbe6766f1d439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.075\n",
      "epoch: 02, loss: 1.048\n",
      "epoch: 03, loss: 1.036\n",
      "epoch: 04, loss: 1.024\n",
      "epoch: 05, loss: 1.015\n",
      "epoch: 06, loss: 1.006\n",
      "epoch: 07, loss: 1.000\n",
      "epoch: 08, loss: 0.994\n",
      "epoch: 09, loss: 0.989\n",
      "epoch: 10, loss: 0.985\n",
      "training with dropout:0.1 l2:0.001 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_336 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bd5b94de4248b692973c514a29ae0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1633e07e85463cb6b0caa9eea6070b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.143\n",
      "epoch: 02, loss: 1.138\n",
      "epoch: 03, loss: 1.134\n",
      "epoch: 04, loss: 1.130\n",
      "epoch: 05, loss: 1.126\n",
      "epoch: 06, loss: 1.123\n",
      "epoch: 07, loss: 1.120\n",
      "epoch: 08, loss: 1.117\n",
      "epoch: 09, loss: 1.115\n",
      "epoch: 10, loss: 1.113\n",
      "training with dropout:0.1 l2:1e-06 lr:0.1\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_340 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a14e004c0e74a79b4a0d98943ffe367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0bbad7c7824912a2a46eecffb31bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.037\n",
      "epoch: 02, loss: 0.936\n",
      "epoch: 03, loss: 0.902\n",
      "epoch: 04, loss: 0.886\n",
      "epoch: 05, loss: 0.882\n",
      "epoch: 06, loss: 0.877\n",
      "epoch: 07, loss: 0.875\n",
      "epoch: 08, loss: 0.875\n",
      "epoch: 09, loss: 0.875\n",
      "epoch: 10, loss: 0.875\n",
      "training with dropout:0.1 l2:1e-06 lr:0.01\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_344 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429fd682924f4ddc95f936673cbb1d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bc56d793df43158d801d640054566e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.065\n",
      "epoch: 02, loss: 1.045\n",
      "epoch: 03, loss: 1.024\n",
      "epoch: 04, loss: 1.012\n",
      "epoch: 05, loss: 0.999\n",
      "epoch: 06, loss: 0.985\n",
      "epoch: 07, loss: 0.974\n",
      "epoch: 08, loss: 0.965\n",
      "epoch: 09, loss: 0.957\n",
      "epoch: 10, loss: 0.950\n",
      "training with dropout:0.1 l2:1e-06 lr:0.0001\n",
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_348 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c01d0e10584f6c8980e708598f3ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aef97ef53194e75a8c739d024df748f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.153\n",
      "epoch: 02, loss: 1.147\n",
      "epoch: 03, loss: 1.141\n",
      "epoch: 04, loss: 1.135\n",
      "epoch: 05, loss: 1.131\n",
      "epoch: 06, loss: 1.126\n",
      "epoch: 07, loss: 1.122\n",
      "epoch: 08, loss: 1.119\n",
      "epoch: 09, loss: 1.115\n",
      "epoch: 10, loss: 1.112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' GRID-SEARCH with bimodal input '''\n",
    "dropout_rates = [0.2, 0.1]\n",
    "l2_lambdas = [1e-03, 1e-06]\n",
    "learning_rates = [0.1, 0.01, 1e-4]\n",
    "bimodal_param_result = grid_search(X_bimodal_train, Y_bimodal_train, dropout_rates, l2_lambdas, learning_rates, NUM_EPOCHS, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow0_col0 {\n",
       "            background-color:  #000004;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow0_col1 {\n",
       "            background-color:  #651a80;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow0_col2 {\n",
       "            background-color:  #030312;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow1_col0 {\n",
       "            background-color:  #331067;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow1_col1 {\n",
       "            background-color:  #271258;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow1_col2 {\n",
       "            background-color:  #1c1044;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow2_col0 {\n",
       "            background-color:  #5f187f;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow2_col1 {\n",
       "            background-color:  #491078;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow2_col2 {\n",
       "            background-color:  #4a1079;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow3_col0 {\n",
       "            background-color:  #491078;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow3_col1 {\n",
       "            background-color:  #59157e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow3_col2 {\n",
       "            background-color:  #601880;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow4_col0 {\n",
       "            background-color:  #1a1042;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow4_col1 {\n",
       "            background-color:  #1e1149;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow4_col2 {\n",
       "            background-color:  #4e117b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow5_col0 {\n",
       "            background-color:  #491078;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow5_col1 {\n",
       "            background-color:  #311165;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow5_col2 {\n",
       "            background-color:  #341069;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow6_col0 {\n",
       "            background-color:  #52137c;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow6_col1 {\n",
       "            background-color:  #420f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow6_col2 {\n",
       "            background-color:  #4e117b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow7_col0 {\n",
       "            background-color:  #341069;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow7_col1 {\n",
       "            background-color:  #671b80;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow7_col2 {\n",
       "            background-color:  #671b80;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow8_col0 {\n",
       "            background-color:  #8b2981;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow8_col1 {\n",
       "            background-color:  #5f187f;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow8_col2 {\n",
       "            background-color:  #752181;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow9_col0 {\n",
       "            background-color:  #a5317e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow9_col1 {\n",
       "            background-color:  #e55064;\n",
       "            color:  #000000;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow9_col2 {\n",
       "            background-color:  #c83e73;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow10_col0 {\n",
       "            background-color:  #bd3977;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow10_col1 {\n",
       "            background-color:  #d5446d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow10_col2 {\n",
       "            background-color:  #e44f64;\n",
       "            color:  #000000;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow11_col0 {\n",
       "            background-color:  #fcfdbf;\n",
       "            color:  #000000;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow11_col1 {\n",
       "            background-color:  #fea36f;\n",
       "            color:  #000000;\n",
       "        }    #T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow11_col2 {\n",
       "            background-color:  #fddc9e;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddb\" ><thead>    <tr>        <th class=\"blank\" ></th>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=3>validation loss</th>    </tr>    <tr>        <th class=\"blank\" ></th>        <th class=\"index_name level1\" >L2 lambda</th>        <th class=\"col_heading level1 col0\" >0.0</th>        <th class=\"col_heading level1 col1\" >1e-06</th>        <th class=\"col_heading level1 col2\" >0.001</th>    </tr>    <tr>        <th class=\"index_name level0\" >learning rate</th>        <th class=\"index_name level1\" >dropout rate</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel0_row0\" class=\"row_heading level0 row0\" rowspan=3>0.0001</th>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row0\" class=\"row_heading level1 row0\" >0.1</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow0_col0\" class=\"data row0 col0\" >1.179</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow0_col1\" class=\"data row0 col1\" >1.098</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow0_col2\" class=\"data row0 col2\" >1.170</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row1\" class=\"row_heading level1 row1\" >0.2</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow1_col0\" class=\"data row1 col0\" >1.131</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow1_col1\" class=\"data row1 col1\" >1.139</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow1_col2\" class=\"data row1 col2\" >1.147</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row2\" class=\"row_heading level1 row2\" >0.5</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow2_col0\" class=\"data row2 col0\" >1.103</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow2_col1\" class=\"data row2 col1\" >1.117</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow2_col2\" class=\"data row2 col2\" >1.117</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel0_row3\" class=\"row_heading level0 row3\" rowspan=3>0.001</th>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row3\" class=\"row_heading level1 row3\" >0.1</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow3_col0\" class=\"data row3 col0\" >1.117</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow3_col1\" class=\"data row3 col1\" >1.107</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow3_col2\" class=\"data row3 col2\" >1.102</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row4\" class=\"row_heading level1 row4\" >0.2</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow4_col0\" class=\"data row4 col0\" >1.148</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow4_col1\" class=\"data row4 col1\" >1.144</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow4_col2\" class=\"data row4 col2\" >1.114</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row5\" class=\"row_heading level1 row5\" >0.5</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow5_col0\" class=\"data row5 col0\" >1.117</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow5_col1\" class=\"data row5 col1\" >1.132</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow5_col2\" class=\"data row5 col2\" >1.130</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel0_row6\" class=\"row_heading level0 row6\" rowspan=3>0.01</th>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row6\" class=\"row_heading level1 row6\" >0.1</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow6_col0\" class=\"data row6 col0\" >1.111</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow6_col1\" class=\"data row6 col1\" >1.121</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow6_col2\" class=\"data row6 col2\" >1.114</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row7\" class=\"row_heading level1 row7\" >0.2</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow7_col0\" class=\"data row7 col0\" >1.130</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow7_col1\" class=\"data row7 col1\" >1.097</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow7_col2\" class=\"data row7 col2\" >1.098</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row8\" class=\"row_heading level1 row8\" >0.5</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow8_col0\" class=\"data row8 col0\" >1.073</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow8_col1\" class=\"data row8 col1\" >1.103</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow8_col2\" class=\"data row8 col2\" >1.088</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel0_row9\" class=\"row_heading level0 row9\" rowspan=3>0.1</th>\n",
       "                        <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row9\" class=\"row_heading level1 row9\" >0.1</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow9_col0\" class=\"data row9 col0\" >1.057</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow9_col1\" class=\"data row9 col1\" >1.013</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow9_col2\" class=\"data row9 col2\" >1.034</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row10\" class=\"row_heading level1 row10\" >0.2</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow10_col0\" class=\"data row10 col0\" >1.042</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow10_col1\" class=\"data row10 col1\" >1.026</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow10_col2\" class=\"data row10 col2\" >1.015</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddblevel1_row11\" class=\"row_heading level1 row11\" >0.5</th>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow11_col0\" class=\"data row11 col0\" >0.914</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow11_col1\" class=\"data row11 col1\" >0.965</td>\n",
       "                        <td id=\"T_b026c7a8_89d4_11ea_9c2e_a683e732cddbrow11_col2\" class=\"data row11 col2\" >0.933</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b8a2e250>"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Visualize ATAC-seq grid search'''\n",
    "param_result_eval = atac_param_result.copy()\n",
    "for i, row in enumerate(atac_param_result):\n",
    "    for j, v in enumerate(row):\n",
    "        if isinstance(v, tf.Tensor):\n",
    "            val = tf.keras.backend.eval(v)\n",
    "            param_result_eval[i][j] = val\n",
    "            \n",
    "\n",
    "atac_grid_search_df = pd.DataFrame(param_result_eval, \n",
    "                             columns=['dropout rate',\n",
    "                                      'L2 lambda',\n",
    "                                      'learning rate',\n",
    "                                      'validation loss'])\n",
    "atac_grid_search_pivot = (atac_grid_search_df\n",
    "                     .pivot_table(values=['validation loss'],\n",
    "                                  columns=['L2 lambda'],\n",
    "                                  index=['learning rate', 'dropout rate']))\n",
    "atac_grid_search_pivot.style.format('{:.3f}').background_gradient(cmap='magma_r',\n",
    "                                                             axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow0_col0 {\n",
       "            background-color:  #08071e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow0_col1 {\n",
       "            background-color:  #21114e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow1_col0 {\n",
       "            background-color:  #000004;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow1_col1 {\n",
       "            background-color:  #20114b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow2_col0 {\n",
       "            background-color:  #5d177f;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow2_col1 {\n",
       "            background-color:  #681c81;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow3_col0 {\n",
       "            background-color:  #7c2382;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow3_col1 {\n",
       "            background-color:  #59157e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow4_col0 {\n",
       "            background-color:  #feb078;\n",
       "            color:  #000000;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow4_col1 {\n",
       "            background-color:  #fcf9bb;\n",
       "            color:  #000000;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow5_col0 {\n",
       "            background-color:  #fcfdbf;\n",
       "            color:  #000000;\n",
       "        }    #T_b0471788_89d4_11ea_9c2e_a683e732cddbrow5_col1 {\n",
       "            background-color:  #feb27a;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddb\" ><thead>    <tr>        <th class=\"blank\" ></th>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>validation loss</th>    </tr>    <tr>        <th class=\"blank\" ></th>        <th class=\"index_name level1\" >L2 lambda</th>        <th class=\"col_heading level1 col0\" >1e-06</th>        <th class=\"col_heading level1 col1\" >0.001</th>    </tr>    <tr>        <th class=\"index_name level0\" >learning rate</th>        <th class=\"index_name level1\" >dropout rate</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel0_row0\" class=\"row_heading level0 row0\" rowspan=2>0.0001</th>\n",
       "                        <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel1_row0\" class=\"row_heading level1 row0\" >0.1</th>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow0_col0\" class=\"data row0 col0\" >1.142</td>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow0_col1\" class=\"data row0 col1\" >1.097</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel1_row1\" class=\"row_heading level1 row1\" >0.2</th>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow1_col0\" class=\"data row1 col0\" >1.174</td>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow1_col1\" class=\"data row1 col1\" >1.099</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel0_row2\" class=\"row_heading level0 row2\" rowspan=2>0.01</th>\n",
       "                        <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel1_row2\" class=\"row_heading level1 row2\" >0.1</th>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow2_col0\" class=\"data row2 col0\" >1.018</td>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow2_col1\" class=\"data row2 col1\" >1.005</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel1_row3\" class=\"row_heading level1 row3\" >0.2</th>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow3_col0\" class=\"data row3 col0\" >0.976</td>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow3_col1\" class=\"data row3 col1\" >1.026</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel0_row4\" class=\"row_heading level0 row4\" rowspan=2>0.1</th>\n",
       "                        <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel1_row4\" class=\"row_heading level1 row4\" >0.1</th>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow4_col0\" class=\"data row4 col0\" >0.719</td>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow4_col1\" class=\"data row4 col1\" >0.632</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddblevel1_row5\" class=\"row_heading level1 row5\" >0.2</th>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow5_col0\" class=\"data row5 col0\" >0.628</td>\n",
       "                        <td id=\"T_b0471788_89d4_11ea_9c2e_a683e732cddbrow5_col1\" class=\"data row5 col1\" >0.717</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c80ced10>"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Visualize ATAC-seq grid search'''\n",
    "rna_param_result_eval = rna_param_result.copy()\n",
    "for i, row in enumerate(rna_param_result):\n",
    "    for j, v in enumerate(row):\n",
    "        if isinstance(v, tf.Tensor):\n",
    "            val = tf.keras.backend.eval(v)\n",
    "            rna_param_result_eval[i][j] = val\n",
    "            \n",
    "rna_grid_search_df = pd.DataFrame(rna_param_result_eval, \n",
    "                             columns=['dropout rate',\n",
    "                                      'L2 lambda',\n",
    "                                      'learning rate',\n",
    "                                      'validation loss'])\n",
    "rna_grid_search_pivot = (rna_grid_search_df\n",
    "                     .pivot_table(values=['validation loss'],\n",
    "                                  columns=['L2 lambda'],\n",
    "                                  index=['learning rate', 'dropout rate']))\n",
    "rna_grid_search_pivot.style.format('{:.3f}').background_gradient(cmap='magma_r',\n",
    "                                                             axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow0_col0 {\n",
       "            background-color:  #010005;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow0_col1 {\n",
       "            background-color:  #000004;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow1_col0 {\n",
       "            background-color:  #060518;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow1_col1 {\n",
       "            background-color:  #000004;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow2_col0 {\n",
       "            background-color:  #9c2e7f;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow2_col1 {\n",
       "            background-color:  #762181;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow3_col0 {\n",
       "            background-color:  #6b1d81;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow3_col1 {\n",
       "            background-color:  #792282;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow4_col0 {\n",
       "            background-color:  #ea5661;\n",
       "            color:  #000000;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow4_col1 {\n",
       "            background-color:  #f7705c;\n",
       "            color:  #000000;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow5_col0 {\n",
       "            background-color:  #fcfdbf;\n",
       "            color:  #000000;\n",
       "        }    #T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow5_col1 {\n",
       "            background-color:  #fec88c;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddb\" ><thead>    <tr>        <th class=\"blank\" ></th>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>validation loss</th>    </tr>    <tr>        <th class=\"blank\" ></th>        <th class=\"index_name level1\" >L2 lambda</th>        <th class=\"col_heading level1 col0\" >1e-06</th>        <th class=\"col_heading level1 col1\" >0.001</th>    </tr>    <tr>        <th class=\"index_name level0\" >learning rate</th>        <th class=\"index_name level1\" >dropout rate</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel0_row0\" class=\"row_heading level0 row0\" rowspan=2>0.0001</th>\n",
       "                        <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel1_row0\" class=\"row_heading level1 row0\" >0.1</th>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow0_col0\" class=\"data row0 col0\" >1.112</td>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow0_col1\" class=\"data row0 col1\" >1.113</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel1_row1\" class=\"row_heading level1 row1\" >0.2</th>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow1_col0\" class=\"data row1 col0\" >1.097</td>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow1_col1\" class=\"data row1 col1\" >1.114</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel0_row2\" class=\"row_heading level0 row2\" rowspan=2>0.01</th>\n",
       "                        <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel1_row2\" class=\"row_heading level1 row2\" >0.1</th>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow2_col0\" class=\"data row2 col0\" >0.950</td>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow2_col1\" class=\"data row2 col1\" >0.985</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel1_row3\" class=\"row_heading level1 row3\" >0.2</th>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow3_col0\" class=\"data row3 col0\" >0.994</td>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow3_col1\" class=\"data row3 col1\" >0.982</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel0_row4\" class=\"row_heading level0 row4\" rowspan=2>0.1</th>\n",
       "                        <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel1_row4\" class=\"row_heading level1 row4\" >0.1</th>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow4_col0\" class=\"data row4 col0\" >0.875</td>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow4_col1\" class=\"data row4 col1\" >0.852</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddblevel1_row5\" class=\"row_heading level1 row5\" >0.2</th>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow5_col0\" class=\"data row5 col0\" >0.740</td>\n",
       "                        <td id=\"T_b88b52b0_89d4_11ea_9c2e_a683e732cddbrow5_col1\" class=\"data row5 col1\" >0.784</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1aba23090>"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Visualize ATAC-seq grid search'''\n",
    "bimodal_param_result_eval = bimodal_param_result.copy()\n",
    "for i, row in enumerate(bimodal_param_result):\n",
    "    for j, v in enumerate(row):\n",
    "        if isinstance(v, tf.Tensor):\n",
    "            val = tf.keras.backend.eval(v)\n",
    "            bimodal_param_result_eval[i][j] = val\n",
    "            \n",
    "bimodal_grid_search_df = pd.DataFrame(bimodal_param_result_eval, \n",
    "                             columns=['dropout rate',\n",
    "                                      'L2 lambda',\n",
    "                                      'learning rate',\n",
    "                                      'validation loss'])\n",
    "bimodal_grid_search_pivot = (bimodal_grid_search_df\n",
    "                     .pivot_table(values=['validation loss'],\n",
    "                                  columns=['L2 lambda'],\n",
    "                                  index=['learning rate', 'dropout rate']))\n",
    "bimodal_grid_search_pivot.style.format('{:.3f}').background_gradient(cmap='magma_r',\n",
    "                                                             axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Predict using validation or test set '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_352 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3e8e4476a64bb880b5af1d83e7459f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a53338eacc4eb5b695d758854d02a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.099\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 02, loss: 1.149\n",
      "epoch: 03, loss: 1.088\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 04, loss: 1.081\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 05, loss: 1.012\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 06, loss: 0.983\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 07, loss: 0.946\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 08, loss: 0.918\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 09, loss: 0.915\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 10, loss: 0.903\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 11, loss: 0.899\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 12, loss: 0.907\n",
      "epoch: 13, loss: 0.902\n",
      "epoch: 14, loss: 0.896\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 15, loss: 0.915\n",
      "epoch: 16, loss: 0.878\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 17, loss: 0.905\n",
      "epoch: 18, loss: 0.899\n",
      "epoch: 19, loss: 0.909\n",
      "epoch: 20, loss: 0.894\n",
      "epoch: 21, loss: 0.912\n",
      "epoch: 22, loss: 0.901\n",
      "epoch: 23, loss: 0.887\n",
      "epoch: 24, loss: 0.888\n",
      "epoch: 25, loss: 0.887\n",
      "epoch: 26, loss: 0.896\n",
      "epoch: 27, loss: 0.920\n",
      "epoch: 28, loss: 0.927\n",
      "epoch: 29, loss: 0.909\n",
      "epoch: 30, loss: 0.904\n",
      "epoch: 31, loss: 0.901\n",
      "epoch: 32, loss: 0.911\n",
      "epoch: 33, loss: 0.910\n",
      "epoch: 34, loss: 0.906\n",
      "epoch: 35, loss: 0.895\n",
      "epoch: 36, loss: 0.888\n",
      "epoch: 37, loss: 0.885\n",
      "epoch: 38, loss: 0.876\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 39, loss: 0.887\n",
      "epoch: 40, loss: 0.901\n",
      "epoch: 41, loss: 0.885\n",
      "epoch: 42, loss: 0.873\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 43, loss: 0.877\n",
      "epoch: 44, loss: 0.880\n",
      "epoch: 45, loss: 0.908\n",
      "epoch: 46, loss: 0.897\n",
      "epoch: 47, loss: 0.889\n",
      "epoch: 48, loss: 0.894\n",
      "epoch: 49, loss: 0.895\n",
      "epoch: 50, loss: 0.929\n",
      "epoch: 51, loss: 0.917\n",
      "epoch: 52, loss: 0.917\n",
      "epoch: 53, loss: 0.899\n",
      "epoch: 54, loss: 0.889\n",
      "epoch: 55, loss: 0.884\n",
      "epoch: 56, loss: 0.893\n",
      "epoch: 57, loss: 0.898\n",
      "epoch: 58, loss: 0.894\n",
      "epoch: 59, loss: 0.892\n",
      "epoch: 60, loss: 0.888\n",
      "epoch: 61, loss: 0.902\n",
      "epoch: 62, loss: 0.906\n",
      "epoch: 63, loss: 0.894\n",
      "epoch: 64, loss: 0.901\n",
      "epoch: 65, loss: 0.903\n",
      "epoch: 66, loss: 0.900\n",
      "epoch: 67, loss: 0.896\n",
      "epoch: 68, loss: 0.899\n",
      "epoch: 69, loss: 0.904\n",
      "epoch: 70, loss: 0.899\n",
      "epoch: 71, loss: 0.895\n",
      "epoch: 72, loss: 0.893\n",
      "epoch: 73, loss: 0.884\n",
      "epoch: 74, loss: 0.887\n",
      "epoch: 75, loss: 0.883\n",
      "epoch: 76, loss: 0.876\n",
      "epoch: 77, loss: 0.877\n",
      "epoch: 78, loss: 0.876\n",
      "epoch: 79, loss: 0.885\n",
      "epoch: 80, loss: 0.880\n",
      "epoch: 81, loss: 0.872\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 82, loss: 0.873\n",
      "epoch: 83, loss: 0.866\n",
      "INFO:tensorflow:Assets written to: models/best_atac_model/assets\n",
      "epoch: 84, loss: 0.873\n",
      "epoch: 85, loss: 0.877\n",
      "epoch: 86, loss: 0.883\n",
      "epoch: 87, loss: 0.883\n",
      "epoch: 88, loss: 0.884\n",
      "epoch: 89, loss: 0.887\n",
      "epoch: 90, loss: 0.892\n",
      "epoch: 91, loss: 0.897\n",
      "epoch: 92, loss: 0.889\n",
      "epoch: 93, loss: 0.891\n",
      "epoch: 94, loss: 0.882\n",
      "epoch: 95, loss: 0.884\n",
      "epoch: 96, loss: 0.884\n",
      "epoch: 97, loss: 0.885\n",
      "epoch: 98, loss: 0.880\n",
      "epoch: 99, loss: 0.881\n",
      "epoch: 100, loss: 0.882\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_DIR = os.path.join('models', 'best_atac_model')\n",
    "NUM_EPOCHS = 100\n",
    "ATAC_BATCH_SIZE = 400\n",
    "# Define hyperparam_config e.g.\n",
    "\n",
    "######################## BEGIN YOUR ANSWER ########################\n",
    "atac_hyperparam_config = {'dropout_rate': 0.5,\n",
    "                     'l2_lambda': 0.001,\n",
    "                     'lr': 0.1}\n",
    "######################### END YOUR ANSWER #########################\n",
    "\n",
    "atac_test_loss = training(\n",
    "    X_atac_train, Y_atac_train, X_atac_test, Y_atac_test,\n",
    "    atac_hyperparam_config, NUM_EPOCHS, ATAC_BATCH_SIZE, save_model=True, model_dir=BEST_MODEL_DIR\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_356 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f991caf32e914bc0b02ad7200743852c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc20c6c81aba472fa2138dfb53272ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.081\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 02, loss: 1.008\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 03, loss: 0.872\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 04, loss: 0.773\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 05, loss: 0.732\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 06, loss: 0.710\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 07, loss: 0.705\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 08, loss: 0.694\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 09, loss: 0.688\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 10, loss: 0.694\n",
      "epoch: 11, loss: 0.688\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 12, loss: 0.689\n",
      "epoch: 13, loss: 0.687\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 14, loss: 0.689\n",
      "epoch: 15, loss: 0.691\n",
      "epoch: 16, loss: 0.686\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 17, loss: 0.684\n",
      "INFO:tensorflow:Assets written to: models/best_rna_model/assets\n",
      "epoch: 18, loss: 0.687\n",
      "epoch: 19, loss: 0.688\n",
      "epoch: 20, loss: 0.689\n",
      "epoch: 21, loss: 0.689\n",
      "epoch: 22, loss: 0.690\n",
      "epoch: 23, loss: 0.692\n",
      "epoch: 24, loss: 0.694\n",
      "epoch: 25, loss: 0.692\n",
      "epoch: 26, loss: 0.692\n",
      "epoch: 27, loss: 0.692\n",
      "epoch: 28, loss: 0.692\n",
      "epoch: 29, loss: 0.692\n",
      "epoch: 30, loss: 0.694\n",
      "epoch: 31, loss: 0.693\n",
      "epoch: 32, loss: 0.694\n",
      "epoch: 33, loss: 0.691\n",
      "epoch: 34, loss: 0.692\n",
      "epoch: 35, loss: 0.693\n",
      "epoch: 36, loss: 0.694\n",
      "epoch: 37, loss: 0.694\n",
      "epoch: 38, loss: 0.692\n",
      "epoch: 39, loss: 0.693\n",
      "epoch: 40, loss: 0.692\n",
      "epoch: 41, loss: 0.693\n",
      "epoch: 42, loss: 0.695\n",
      "epoch: 43, loss: 0.695\n",
      "epoch: 44, loss: 0.694\n",
      "epoch: 45, loss: 0.695\n",
      "epoch: 46, loss: 0.695\n",
      "epoch: 47, loss: 0.698\n",
      "epoch: 48, loss: 0.698\n",
      "epoch: 49, loss: 0.698\n",
      "epoch: 50, loss: 0.697\n",
      "epoch: 51, loss: 0.698\n",
      "epoch: 52, loss: 0.699\n",
      "epoch: 53, loss: 0.700\n",
      "epoch: 54, loss: 0.699\n",
      "epoch: 55, loss: 0.699\n",
      "epoch: 56, loss: 0.698\n",
      "epoch: 57, loss: 0.699\n",
      "epoch: 58, loss: 0.701\n",
      "epoch: 59, loss: 0.700\n",
      "epoch: 60, loss: 0.702\n",
      "epoch: 61, loss: 0.701\n",
      "epoch: 62, loss: 0.701\n",
      "epoch: 63, loss: 0.701\n",
      "epoch: 64, loss: 0.701\n",
      "epoch: 65, loss: 0.704\n",
      "epoch: 66, loss: 0.703\n",
      "epoch: 67, loss: 0.702\n",
      "epoch: 68, loss: 0.701\n",
      "epoch: 69, loss: 0.702\n",
      "epoch: 70, loss: 0.703\n",
      "epoch: 71, loss: 0.702\n",
      "epoch: 72, loss: 0.703\n",
      "epoch: 73, loss: 0.705\n",
      "epoch: 74, loss: 0.702\n",
      "epoch: 75, loss: 0.703\n",
      "epoch: 76, loss: 0.704\n",
      "epoch: 77, loss: 0.704\n",
      "epoch: 78, loss: 0.705\n",
      "epoch: 79, loss: 0.704\n",
      "epoch: 80, loss: 0.705\n",
      "epoch: 81, loss: 0.704\n",
      "epoch: 82, loss: 0.704\n",
      "epoch: 83, loss: 0.704\n",
      "epoch: 84, loss: 0.705\n",
      "epoch: 85, loss: 0.705\n",
      "epoch: 86, loss: 0.705\n",
      "epoch: 87, loss: 0.706\n",
      "epoch: 88, loss: 0.706\n",
      "epoch: 89, loss: 0.706\n",
      "epoch: 90, loss: 0.706\n",
      "epoch: 91, loss: 0.707\n",
      "epoch: 92, loss: 0.706\n",
      "epoch: 93, loss: 0.707\n",
      "epoch: 94, loss: 0.706\n",
      "epoch: 95, loss: 0.707\n",
      "epoch: 96, loss: 0.705\n",
      "epoch: 97, loss: 0.705\n",
      "epoch: 98, loss: 0.705\n",
      "epoch: 99, loss: 0.706\n",
      "epoch: 100, loss: 0.705\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_DIR = os.path.join('models', 'best_rna_model')\n",
    "RNA_BATCH_SIZE = 400\n",
    "\n",
    "rna_hyperparam_config = {'dropout_rate': 0.2,\n",
    "                     'l2_lambda': 1e-06,\n",
    "                     'lr': 0.1}\n",
    "\n",
    "rna_test_loss = training(\n",
    "    X_rna_train, Y_rna_train, X_rna_test, Y_rna_test,\n",
    "    rna_hyperparam_config, NUM_EPOCHS, RNA_BATCH_SIZE, save_model=True, model_dir=BEST_MODEL_DIR\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting initial loss\n",
      "WARNING:tensorflow:Layer dense_360 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5df50ea760943799238d8daeb262f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c52d659c9d4fb48c496be20dba3e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training Steps', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, loss: 1.004\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 02, loss: 0.884\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 03, loss: 0.790\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 04, loss: 0.758\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 05, loss: 0.740\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 06, loss: 0.730\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 07, loss: 0.722\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 08, loss: 0.720\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 09, loss: 0.718\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 10, loss: 0.715\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 11, loss: 0.715\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 12, loss: 0.713\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 13, loss: 0.712\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 14, loss: 0.712\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 15, loss: 0.711\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 16, loss: 0.710\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 17, loss: 0.714\n",
      "epoch: 18, loss: 0.711\n",
      "epoch: 19, loss: 0.709\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 20, loss: 0.708\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 21, loss: 0.706\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 22, loss: 0.706\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 23, loss: 0.706\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 24, loss: 0.705\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 25, loss: 0.705\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 26, loss: 0.704\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 27, loss: 0.704\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 28, loss: 0.704\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 29, loss: 0.704\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 30, loss: 0.703\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 31, loss: 0.703\n",
      "epoch: 32, loss: 0.703\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 33, loss: 0.702\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 34, loss: 0.702\n",
      "epoch: 35, loss: 0.702\n",
      "epoch: 36, loss: 0.702\n",
      "epoch: 37, loss: 0.703\n",
      "epoch: 38, loss: 0.703\n",
      "epoch: 39, loss: 0.703\n",
      "epoch: 40, loss: 0.703\n",
      "epoch: 41, loss: 0.702\n",
      "epoch: 42, loss: 0.702\n",
      "epoch: 43, loss: 0.703\n",
      "epoch: 44, loss: 0.702\n",
      "epoch: 45, loss: 0.702\n",
      "epoch: 46, loss: 0.703\n",
      "epoch: 47, loss: 0.702\n",
      "epoch: 48, loss: 0.702\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 49, loss: 0.702\n",
      "epoch: 50, loss: 0.702\n",
      "epoch: 51, loss: 0.702\n",
      "epoch: 52, loss: 0.702\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 53, loss: 0.702\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 54, loss: 0.702\n",
      "epoch: 55, loss: 0.702\n",
      "epoch: 56, loss: 0.699\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 57, loss: 0.699\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 58, loss: 0.698\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 59, loss: 0.698\n",
      "INFO:tensorflow:Assets written to: models/best_bimodal_model/assets\n",
      "epoch: 60, loss: 0.698\n",
      "epoch: 61, loss: 0.699\n",
      "epoch: 62, loss: 0.699\n",
      "epoch: 63, loss: 0.699\n",
      "epoch: 64, loss: 0.699\n",
      "epoch: 65, loss: 0.699\n",
      "epoch: 66, loss: 0.699\n",
      "epoch: 67, loss: 0.699\n",
      "epoch: 68, loss: 0.700\n",
      "epoch: 69, loss: 0.700\n",
      "epoch: 70, loss: 0.700\n",
      "epoch: 71, loss: 0.700\n",
      "epoch: 72, loss: 0.700\n",
      "epoch: 73, loss: 0.700\n",
      "epoch: 74, loss: 0.700\n",
      "epoch: 75, loss: 0.701\n",
      "epoch: 76, loss: 0.701\n",
      "epoch: 77, loss: 0.701\n",
      "epoch: 78, loss: 0.701\n",
      "epoch: 79, loss: 0.701\n",
      "epoch: 80, loss: 0.701\n",
      "epoch: 81, loss: 0.701\n",
      "epoch: 82, loss: 0.701\n",
      "epoch: 83, loss: 0.701\n",
      "epoch: 84, loss: 0.701\n",
      "epoch: 85, loss: 0.701\n",
      "epoch: 86, loss: 0.701\n",
      "epoch: 87, loss: 0.701\n",
      "epoch: 88, loss: 0.702\n",
      "epoch: 89, loss: 0.702\n",
      "epoch: 90, loss: 0.702\n",
      "epoch: 91, loss: 0.702\n",
      "epoch: 92, loss: 0.702\n",
      "epoch: 93, loss: 0.702\n",
      "epoch: 94, loss: 0.701\n",
      "epoch: 95, loss: 0.702\n",
      "epoch: 96, loss: 0.702\n",
      "epoch: 97, loss: 0.702\n",
      "epoch: 98, loss: 0.702\n",
      "epoch: 99, loss: 0.702\n",
      "epoch: 100, loss: 0.702\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_DIR = os.path.join('models', 'best_bimodal_model')\n",
    "BIMODAL_BATCH_SIZE = 300\n",
    "\n",
    "bimodal_hyperparam_config = {'dropout_rate': 0.2,\n",
    "                     'l2_lambda': 1e-06,\n",
    "                     'lr': 0.1}\n",
    "\n",
    "bimodal_test_loss = training(\n",
    "    X_bimodal_train, Y_bimodal_train, X_bimodal_test, Y_bimodal_test,\n",
    "    rna_hyperparam_config, NUM_EPOCHS, BIMODAL_BATCH_SIZE, save_model=True, model_dir=BEST_MODEL_DIR\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model_dir):\n",
    "    \"\"\"\n",
    "    Load a trained model and predict the learned embedding for\n",
    "    examples in x.\n",
    "    \n",
    "    Arguments:\n",
    "      x: input data of size [n_datapoints, n_features]\n",
    "      model_dir: location of saved model\n",
    "      \n",
    "    Returns:\n",
    "      y_embed: 2 dimension t-SNE embeddings for x\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################## BEGIN YOUR ANSWER ########################\n",
    "    model = K.models.load_model(model_dir)\n",
    "    y_pred = model.predict(x)\n",
    "    #convert to one-hot representation\n",
    "    y_output = tf.one_hot(tf.nn.top_k(y_pred).indices, tf.shape(y_pred)[1])\n",
    "\n",
    "    ######################### END YOUR ANSWER #########################\n",
    "\n",
    "    return tf.squeeze(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7605634"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Evaluate model trained ONLY on ATAC-seq input '''\n",
    "y_atac_pred = predict(X_atac_test, model_dir='models/best_atac_model')\n",
    "\n",
    "''' Get accuracy '''\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "_ = m.update_state(y_atac_pred, Y_atac_test)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7928349"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Evaluate model trained ONLY on RNA-seq input '''\n",
    "y_rna_pred = predict(X_rna_test, model_dir='models/best_rna_model')\n",
    "\n",
    "''' Get accuracy '''\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "_ = m.update_state(y_rna_pred, Y_rna_test)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80981594"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Evaluate model trained on bimodal input (concatenation of ATAC + RNA) '''\n",
    "y_bimodal_pred = predict(X_bimodal_test, model_dir='models/best_bimodal_model')\n",
    "\n",
    "''' Get accuracy '''\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "_ = m.update_state(y_bimodal_pred, Y_bimodal_test)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
